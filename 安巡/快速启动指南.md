# 安巡系统 - 快速启动指南

## 🚀 一键启动（推荐）

使用提供的启动脚本可以自动启动所有必需的服务：

```bash
python start_services.py
```

### 启动流程
1. 检查Ollama服务状态
2. 启动后端API服务器（端口5000）
3. 启动前端Streamlit应用（端口8506）
4. 验证所有服务正常运行

### 访问地址
- **前端应用**: http://localhost:8506
- **后端API**: http://localhost:5000
- **Ollama服务**: http://localhost:11434

## 📋 前置要求

### 1. 安装Ollama
```bash
# 下载并安装Ollama
# 访问 https://ollama.ai 下载对应系统版本

# 启动Ollama服务
ollama serve

# 下载模型（在新终端中运行）
ollama pull qwen2.5:7b
```

### 2. 安装Python依赖
```bash
# 安装前端依赖
cd front
pip install -r requirements.txt

# 安装后端依赖
cd ../back
pip install -r requirements.txt
```

## 🔧 手动启动（备选方案）

如果一键启动失败，可以手动启动各个服务：

### 1. 启动Ollama服务
```bash
ollama serve
```

### 2. 启动后端API服务器
```bash
cd back
python api_server.py
```

### 3. 启动前端应用
```bash
cd front
streamlit run app.py --server.port=8506
```

## 🐛 故障排除

### 问题1: API调用失败
**症状**: 对话功能不工作，显示"API调用失败"

**解决方案**:
1. 确认Ollama服务正在运行：`ollama serve`
2. 确认后端API服务器正在运行：访问 http://localhost:5000/api/health
3. 检查模型是否已下载：`ollama list`

### 问题2: 流量捕获失败
**症状**: 网络流量捕获功能不工作

**解决方案**:
1. 确认后端服务正在运行
2. 检查网络接口权限（可能需要管理员权限）
3. 确认traffic_analyzer.py模块正常工作

### 问题3: session_state错误
**症状**: Streamlit显示session_state相关错误

**解决方案**:
1. 刷新浏览器页面
2. 清除浏览器缓存
3. 重启前端应用

### 问题4: 系统状态检查缓慢
**症状**: 切换页面时系统响应慢

**解决方案**:
- 系统已优化状态检查，缓存时间为60秒
- 如仍然缓慢，可以重启应用

## 📊 功能说明

### 智能对话
- 支持与AI模型对话
- 具备上下文记忆功能
- 支持流量包上传分析

### 流量分析
- 实时网络流量捕获
- 流量包文件分析
- AI驱动的安全分析
- 可视化分析结果

### 教育资源
- 网络安全知识库
- 安全测试题目
- 快速安全提示
- 紧急联系方式

## 🔒 安全注意事项

1. **网络权限**: 流量捕获功能可能需要管理员权限
2. **防火墙**: 确保端口5000和8506未被防火墙阻止
3. **数据隐私**: 捕获的网络数据仅在本地处理，不会上传到外部服务器

## 📞 技术支持

如果遇到问题，请检查：
1. 所有依赖是否正确安装
2. Ollama服务是否正常运行
3. 网络连接是否正常
4. 系统权限是否足够

---

**安巡团队** - 让网络安全更智能、更有温度